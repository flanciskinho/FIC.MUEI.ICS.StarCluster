\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}

\usepackage{geometry}               % Márgenes del documento
\usepackage{amsfonts}
\usepackage{graphicx}

\usepackage[nottoc]{tocbibind}

\usepackage{color}
\usepackage[pdftex, colorlinks=true, linkcolor=blue, urlcolor=red, filecolor=magenta, citecolor=blue]{hyperref}
\definecolor{gray97}{gray}{.97}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}


\usepackage{listings}
\usepackage[usenames,dvipsnames]{xcolor}
\colorlet{keyword}{blue!100!black!80}
\colorlet{STD}{Lavender}
\colorlet{comment}{green!80!black!90}

\lstdefinestyle{miniBash}{
	language     = bash,
	tabsize=3,
	numbers=left, % Donde se situan los numeros
	frame=none, % Se pone un marco
	showstringspaces=false, % No muestre el cuadrado en los espacios de los strings
	backgroundcolor = \color{gray97},
	basicstyle   = \footnotesize \ttfamily,
	keywordstyle = [1]\color{keyword}\bfseries,
	keywordstyle = [2]\color{STD}\bfseries,
	breaklines=true,                % sets automatic line breaking
	commentstyle = \color{comment}
}

\lstdefinestyle{C}{
	language     = C,
	tabsize=3,
	numbers=left, % Donde se situan los numeros
	frame=single, % Se pone un marco
	showstringspaces=false, % No muestre el cuadrado en los espacios de los strings
	backgroundcolor = \color{gray97},
	basicstyle   = \footnotesize \ttfamily,
	keywordstyle = [1]\color{keyword}\bfseries,
	keywordstyle = [2]\color{STD}\bfseries,
	breaklines=true,                % sets automatic line breaking
	commentstyle = \color{comment}
}

\geometry{a4paper}                  % Tamaño y márgenes del documento
\geometry{left=2.5cm,top=2.5cm}
\geometry{bottom=2.5cm,right=2.5cm}

\geometry{driver=dvips,pdftex} % ???
\setcounter{secnumdepth}{5}    % ???
\setcounter{tocdepth}{5}       % ???

\setlength{\parskip}{\baselineskip} % Espacio entre párrafos (una linea en blanco)

%--------------------------------------------------------------------------
\title{\textbf{Informática como servicio}
\\ \textbf{\emph{StarCluster}}
}
\author{Francisco Abel Cedrón Santaeufemia \and \textit{francisco.cedron@udc.es}}
\date{} %Asi no inserta la fecha

\addto\captionsspanish{
\def\tablename{Tabla}
\def\listtablename{Índice de tablas}
}

\begin{document}
\maketitle % Pone titulo, autor
\renewcommand{\abstractname}{Abstract} % El nombre que aparece al principio del abstract
\begin{abstract}
	En este documento se puede leer el trabajo realizado por el alumno para la práctica optativa de informática como servicio. El objetivo de este trabajo es poder mostrar con un código altamente paralelizable la importancia de disponer de una gran potencia de cálculo empleando varias máquinas para la ejecución de un programa. Además se implementará un sistema de colas de trabajo en el que se puedan poner procesos en una cola de ejecución para ejecutarse de manera ordenada.
\end{abstract}
\renewcommand{\contentsname}{} % Lo que pone en el indice
{\setlength{\parskip}{0mm} \tableofcontents} % Para que no ponga espacios entre las lineas de indice
%\vspace{1cm} % Para dejar un espacio con respecto a la tabla

\section{Descripción de la práctica}
	El objetivo de esta práctica es poder crear un entorno para poder ejecutar programas paralelos con diversas máquinas. Para ello empezaremos por crearnos un programa paralelo (sección\ref{sec:ParallelProgram} )para poder ver las ventajas de poder ejecutar la aplicación creada en diferentes entornos como puede ser en local (sección \ref{sec:ejStarCluster}) o en un cluster (sección \ref{sec:ejStarCluster}).
	
	Para la creación de un cluster se hará sobre Amazon EC2 usando la herramienta StarCluster (sección \ref{sec:InstallStarCluster}).
	
	Para acabar se explicará en un apartado de conclusiones (sección \ref{sec:Conclusion}) los problemas de que se tuvieron al instalar StarCluster y un comparativa de los resultados obtenidos.
	
\section{Programa paralelo}\label{sec:ParallelProgram}
{\setlength{\parskip}{0mm}
	La aplicación que se desarrolló en este trabajo es una manera de escalar una imagen digital. Dicho proceso se hace mediante la siguiente transformación:
\begin{equation}
R(x, y) = A \left( \frac{x}{i_x}, \frac{y}{i_y} \right)
\end{equation}
donde \emph{A(x, y)} es la imagen de entrada mientras que \emph{R(x, y)} es la salida y donde el valor de $i_x$ indica el incremento en el eje horizontal 
e $i_y$ el incremento en el eje vertical. El problema que puede ocasionar dicha transformación es que puede dar valores de números decimales en 
los índices con lo que intenta referenciar un valor de un pixel que no existe en una imagen. Para poder solucionar este problema se emplea la interpolación. 
Los métodos más comunes de interpolar una imagen son los siguientes\cite{WOODS}:
\begin{enumerate} {\setlength{\parskip}{0mm}
	\item Interpolación por el vecino más próximo.
	\item Interpolación bilineal.
	\item Interpolación bicúbica.
} \end{enumerate}
}

	La interpolación por el vecino más próximo resuelve el problema cogiendo el valor del pixel más próximo, siendo una manera sencilla de implementar y con una rápida ejecución pero consigue un efecto pixelado en los resultados. Para poder mejorar los resultados obtenidos se puede emplear la interpolación bilineal que para obtener el valor de un pixel que no existe usa los cuatro reales que están a su alrededor y calcula una media ponderada consiguiendo así un mejor resultado visual en la imagen de salida, pero con un tiempo de ejecución mucho mayor. Si aún así se necesita unos mejores resultados, se puede emplear una interpolación bicúbica que requiere el uso de 16 pixels. El resultado que se consigue con la interpolación bicúbica es eliminar los efectos ``cuadriculados'' que se obtienen empleando con la interpolación bilineal, efecto que se consigue al emplear 16 pixels, en vez de 4 al calcular el valor de un pixel resultante (a pesar del gran coste computacional que eso conlleva). Para poder ver el resultado obtenido con los distintos tipos de interpolación se puede ver el resultado en la figura \ref{fig:interpolationExample}.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\textwidth]{img/example.png}
  \caption{Resultados obtenidos al emplear distintas interpolaciones. De izquierda a derecha y de arriba a abajo:
  a) imagen original, b) zoom 10x interpolación por vecino más próximo, c) zoom 10x interpolación bilineal, d) zoom 10x interpolación bicúbica.}
  \label{fig:interpolationExample}
\end{figure}

{\setlength{\parskip}{0mm}
El algoritmo que se desarrollará en este trabajo es el de interpolación bicúbica, que como hemos dicho emplea dos interpolaciones cúbicas. En el caso de una sola dimensión, la interpolación cúbica consiste en trazar una cúbica entre los 4 puntos más próximos (2 a la izquierda y 2 a la derecha).
\begin{equation}
\label{eq:interpolacionCubica}
f(x) = c_0 x^3 + c_1 x^2 + c_2 x + c_3
\end{equation}
para aplicar la interpolación bicúbica se necesita hacer dos interpolaciones cúbicas:
\begin{enumerate}{\setlength{\parskip}{0mm}
	\item Interpolación cúbica \emph{horizontal}, en las filas existentes (usando 4 puntos).
	\item Interpolación cúbica \emph{vertical} en todo el espacio usando 4 puntos (empleando la anterior interpolación).
}\end{enumerate}
}

En este tipo de interpolación, un punto \emph{($p_x$, $p_y$)} emplea 16 pixels circundantes. Así tenemos que el valor del punto se puede calcular como una media ponderada de los 4x4 pixels circundantes. Para calcular el punto interpolado hay que realizar:
\begin{equation}
	A'(p_x ,p_y ) = \sum_{n = -1..2} \sum_{m=-1..2} A(i+n, j+m)P(n-a)P(b-m)
\end{equation}
siendo
\begin{displaymath}
	\begin{matrix}	
		P(k) = \frac{1}{6}(C(k+2)^3 - 4C(k+1)^3 + 6C(k)^3 -4C(k-1)^3) \\
		C(k) = max\lbrace 0, k \rbrace
	\end{matrix}
\end{displaymath}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.45\textwidth]{img/interpolation.png}
  \caption{Parámetros empleados para realizar la interpolación bicúbica}
  \label{fig:interpolation}
\end{figure}



Un problema que hay que abordar en cuanto al uso del tipo de interpolación, es saber qué pixels se van emplear en caso de que se necesite calcular valores que estén próximos al borde, para ello se pueden usar dos métodos:{\setlength{\parskip}{0mm}
\begin{enumerate}{\setlength{\parskip}{0mm}
	\item Usar ceros como valor, siempre que se necesite un valor que esté fuere de la matriz de la imagen.
	\item Usar un efecto espejo en los bordes de la imagen.
}\end{enumerate}
}En este trabajo se implementará la segunda opción.

\subsection{Creación del programa}
Lo primero que se necesita es poder simular una imagen. Para ello se realiza una simulación tal y como se muestra en el siguiente trozo de código:

\begin{lstlisting}[style=C]
void getImage(char *matrix, int height, int width) {
	int i,j;
	 
	for (i = 0; i < height; i++)
		for (j = 0; j < width; j++)
			matrix INDEX(i, j, width) = rand() % 256;//gray level
}
\end{lstlisting}

Al utilizar el método de escalado de una imagen descrito anteriormente, nos encontraríamos con un problema al intentar hallar el valor de un pixel en los bordes de la imagen. Para solucionar este problema se llevó a cabo una de las soluciones encontradas en la literatura, se emplea el método de creación de bordes alrededor de la imagen con efecto espejo, añadiendo dos filas en la parte superior e inferior y también dos columnas a la izquierda y derecha de la imagen. 
En la figura \ref{fig:ejemploMatrices} se muestra un ejemplo representativo de la matriz original y la aumentada.

\begin{figure}[h]
\centering
\begin{lstlisting}
                               236  74  74 236  41 205 186 171 171 186
                               198 103 103 198 105 115  81 255 255  81 
  103 198 105 115  81 255      198 103 103 198 105 115  81 255 255  81   
   74 236  41 205 186 171      236  74  74 236  41 205 186 171 171 186
  242 251 227  70 124 194      251 242 242 251 227  70 124 194 194 124
   84 248  27 232 231 141      248  84  84 248  27 232 231 141 141 231
  118  90  46  99  51 159       90 118 118  90  46  99  51 159 159  51 
                                90 118 118  90  46  99  51 159 159  51
                               248  84  84 248  27 232 231 141 141 231	
\end{lstlisting}
\caption{Izquierda: Imagen original. Derecha: Imagen espejo.}
\label{fig:ejemploMatrices}
\end{figure}

Una vez que tenemos la imagen original y se han creado los ``bordes'', se realiza el proceso de escalado con la siguiente función:

\begin{lstlisting}[style=C]
int ** getScale(char *mirror, char *result, int height, int width, int delay, float ix, float iy) {
	//Asignacion de memoria para la imagen resultante
	int **tmp = malloc(sizeof(int *)*(height*iy));
	
	if (tmp == NULL)
		exit_msg("getImage: cannot allocate memory (1)");
	
	int i,j;
	for (i = 0; i < height*iy; i++) {
		if ((tmp[i] = malloc(sizeof(int) * (width*ix))) == NULL)
			exit_msg("getImage: cannot allocate memory (2)");
	}

	int cnt = 0;
	int n, m;
	float sum;
	float pn, pm;
	float a, b;
	//Iniciar el proceso de convolucion
	for (i = 0; i < height*iy; i++) {//Bucle a paralelizar con MPI
		for (j = 0; j < width*ix; j++) {
			//tmp[i][j] = MAX(0, cnt++);
			sum = 0.0f;
			a = ((float) i)/ix  - ((int) i/ix);
			b = ((float) j)/iy  - ((int) j/iy);
			for (n = -1; n < 3; n++) { //Ambos bucles con SSE
				for (m = -1; m < 3; m++) {
					pn = Pk(n - a);
					pm = Pk(b - m);
					sum += mirror[(int) (i/ix+delay+n)][(int) (j/iy+delay+m)]*pn*pm;
				}
			}
			tmp[i][j] = (int) sum;	
		}
	}
	return tmp;
}
\end{lstlisting}

\subsection{Técnicas de paralelización utilizadas}
\subsubsection{Paso de mensajes con MPI}
Esta técnica se aplicó al bucle más externo. Con ella se pretendía dividir la carga computacional que crea este segmento de código entre varios procesadores, de manera que cada uno de ellos realizase una parte del cálculo, siendo así más rápida la terminación del programa. El motivo de realizar este reparto es dividir las filas de la imagen original para que cada proceso MPI se encargue de escalar su trozo de imagen, tal y como se muestra en la figura \ref{fig:repartoFilas}\cite{MPICH2, OpenMPI}.

\begin{figure}[h]
        \centering
        \includegraphics[angle=0, height=0.3\textheight]{img/repartoFilas.pdf}
        \caption{Imagen divida en filas. La región grisácea representa el espejo que se creó para dicha imagen. La imagen original está formada por los píxels de color azul, naranja y rojo. Como se puede visualizar, la imagen está dividida en tres regiones, que son las que cada proceso MPI se encargaría de escalar.}
        \label{fig:repartoFilas}
\end{figure}

\begin{figure}[h]
        \centering
        \includegraphics[angle=0, height=0.3\textheight]{img/repartoMPI.pdf}
        \caption{Cada color de la imagen original (azul, naranja o rojo) son las filas que tendrá que escalar cada proceso, pero para realizar el escalado necesita unos ``bordes'' para poder realizar la interpolación. Lo que se enviará a cada proceso MPI está delimitado con un rectángulo de líneas punteadas (mostrando así que hay trozos de la imagen que pueden estar en varios procesos).}
        \label{fig:repartoMPI}
\end{figure}

Para poder repartir la matriz de la imagen entre los distintos procesos se utilizaron las funciones \emph{MPI\_Scatterv} y \emph{MPI\_Gatherv}. El motivo de su uso fue que los segmentos de matriz enviados a cada proceso estaban solapados, y estas funciones permiten indicar al padre en qué punto empieza la parte de la matriz que debe enviar a un proceso en concreto, así como el tamaño a enviar. Además, este motivo fue la razón de que el reparto de trabajo fuese consecutivo, porque en el caso de ser cíclico, se tendría que enviar toda la imagen a todos los procesos. En la figura \ref{fig:repartoMPI} se puede ver como lo que se envía a cada proceso MPI está solapado con lo que se envía a otros procesos.

El motivo de tener que enviar a cada proceso partes solapadas de la matriz se explica por el mismo motivo de la creación de la matriz espejo, es decir, la parte que debe calcular cada proceso también tiene pixels en los ``bordes'', que necesitarán sus pixels circundantes para calcular su nuevo valor.

Para que se pueda repartir a cada proceso la información necesaria se debe poder indicar las variables necesarias para llamar a las funciones \emph{MPI\_Scatterv} y \emph{MPI\_Gatherv} (la figura \ref{fig:scatterGather} muestra dichas funciones), con lo que en la versión de programa paralelo se han creado unas funciones que indican a las funciones de envío y recepción de MPI los detalles necesarios. Esas funciones se pueden ver en la figura \ref{cod:fill_to_MPI}.

\begin{figure}[h]
\begin{lstlisting}[style=C]
MPI_Scatterv(mirror, size_send_child, aux_split,
	MPI_CHAR, work, size_send_child[myid],
	MPI_CHAR, 0, MPI_COMM_WORLD);
			
MPI_Gatherv(result, size_send_dad[myid], MPI_CHAR,
	scale, size_send_dad, aux_begin,
	MPI_CHAR, 0, MPI_COMM_WORLD);
\end{lstlisting}
\caption{Funciones \emph{MPI\_Scatterv} y \emph{MPI\_Gatherv}}
\label{fig:scatterGather}
\end{figure}

\begin{figure}[h]
\begin{lstlisting}[style=C]
#define MIN(x,y) ((x<y)?x:y)

//Funcion para indicar las filas con las que trabajara cada proceso
void fill_aux_rows(int *array, int size, int height, int delay) {
	int rows = height / size;	
	if (height % size != 0)
		rows++;
	
	int i;
	for (i = 0; i < size; i++)
		array[i] = MIN(rows, height-i*rows) + 2*delay;
}

//Funcion para conocer el numero de elementos a enviar a cada proceso
void fill_size_send_child(int *fill, int size, int *rows, int width, int delay) {
	int i;
	
	for (i = 0; i < size; i++)
		fill[i] = rows[i]* (width + 2*delay);
}

//Funcion para conocer desde donde se empieza a enviar a cada proceso
void fill_aux_split(int *fill, int size, int stride, int width, int delay) {
	int i;
	
	for (i = 0; i < size; i++) 
		fill[i] = i * (stride - (width + delay*2)*4);
}

//Funcion para conocer el tamano de la respuesta que enviara cada proceso
void fill_size_send_dady(int *fill, int size, int *rows, int width, float ix, float iy, int delay) {
	int i;
	
	for (i = 0; i < size; i++)
		fill[i] = (rows[i] -(2*delay))*iy * (width*ix);
}

//Funcion que indica al proceso que almacena el resultado donde tiene que colocar los resultados parciales
void fill_aux_begin(int *fill, int size, int bytes) {
	int i;
	
	for (i = 0; i < size; i++)
		fill[i] = bytes*i;
}
\end{lstlisting}
\caption{Funciones para necesarias para rellenar las variables que necesitan las variables que se pasan por parámetros a las funciones \emph{MPI-Gatherv} y \emph{MPI-Scatherv}}
\label{cod:fill_to_MPI}
\end{figure}

\clearpage

\section{Ejecución del  programa paralelo en local}\label{sec:ejLocal}
	Para empezar a ver los beneficios de la paralelización se puede ejecutar el programa en cualquier ordenador personal\footnote{Es necesario que tenga un procesador multicore para que se pueda apreciar la diferencia de tiempos}. La máquina en la que se va a probar tiene un procesador Intel Core i7-4820k\footnote{\url{http://ark.intel.com/es-es/products/77781/Intel-Core-i7-4820K-Processor-10M-Cache-up-to-3_90-GHz}} de 4 núcleos en el que está montado un sistema operativo Ubuntu 14.04.
	
	Lo primero que tenemos que hacer es instalar un versión de MPI:
\begin{lstlisting}[style=minibash]
$ wget http://www.open-mpi.org/software/ompi/v1.8/downloads/openmpi-1.8.3.tar.gz
$ tar -xvf openmpi-1.8.3.tar.gz
$ cd openmpi-1.8.3
$ ./configure --prefix="/home/$USER/.openmpi
$ make; sudo make install
$ echo 'export PATH=$PATH:/home/$USER/.openmpi/bin' >> /home/$USER/.bashrc
$ echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/$USER/.openmpi/lib' >> /home/$USER/.bashrc
$ source /home/$USER/.bashrc
\end{lstlisting}

	Una vez que tenemos configurado el entorno para ejecutar los programas en paralelo procedemos a compilar el programa con la siguiente instrucción:
\begin{lstlisting}[style=minibash]
$ mpicc paralelizado.c -lm -O3
\end{lstlisting}
	Y con eso podremos ejecutarlo indicando el número de procesos que se quieren emplear, que en este caso irá de 1 hasta 4.
\begin{lstlisting}[style=minibash]
$ mpirun -np 1 ./a.out 1500 1500 4 4
$ mpirun -np 2 ./a.out 1500 1500 4 4
$ mpirun -np 3 ./a.out 1500 1500 4 4
$ mpirun -np 4 ./a.out 1500 1500 4 4
\end{lstlisting}

	Los resultados obtenidos son los que aparecen en la tabla \ref{tab:resultadosLocal} en donde al ejecutar con más de un proceso se puede ver que no se consigue un 100\% en la eficiencia (debido a la comunicaciones entre procesos), pero podemos ver tanto en el tiempo como en el \textit{speedup} que merece la pena ejecutarse de manera paralelizada.
	
\begin{table}[h]
	\begin{center}
		\caption{Resultados de ejecutar el programa paralelo en local con 1, 2, 3 y 4 procesos.}
		\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Procesos} &
\textbf{Tiempo} &
\textbf{Speedup} &
\textbf{Eficiencia} \\ \hline
\hline
1 & 126,81 sg & 1          & 100\% \\ \hline
2 & 63,608 sg & 1,9936 & 99,68\% \\ \hline
3 & 43,286 sg & 2,9295 & 97,65\% \\ \hline
4 & 32,582 sg & 3,8920 & 97,30\% \\ \hline
		\end{tabular}
		\label{tab:resultadosLocal}
	\end{center}
\end{table}

	Aunque esta manera de ejecutar los programas es válida, podría darse el caso de querer lanzar más programas a la vez y el resultado de los tiempos podría no ser adecuado debido a que cada core del procesador tendría que repartir su tiempo de ejecución entre varios procesos de tal manera que los tiempos no serían realistas. Además no se estaría aprovechando una de las características que nos ofrece MPI, el poder ejecutar un mismo programa en varias máquinas para sumar potencia de cálculo.

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%% Seccion final del documento %%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Creación de un cluster en Amazon EC2 con StarCluster}\label{sec:InstallStarCluster}
	En apartados anteriores vimos que hay programas que se puede paralelizar y que se pueden obtener mejoras de tiempo con su ejecución paralela, pero que su ejecución debe de ser de manera ordenada para que no todos los trabajos se ejecuten a la vez y sobrecarguen la máquina.

	El objetivo de este trabajo es poder implementar un servicio de colas similar al que se puede acceder a las máquinas del CESGA. Estas colas son  sistemas de acceso ordenado de los distintos trabajos a los recursos de un servidor, de tal manera que su finalidad es permitir que los recursos se compartan de forma equitativa entre todos los usuarios y que el servidor no se sobrecargue intentando ejecutar todos los programas a la vez. Cuando los trabajos que se encuentran en cola exceden los recursos disponibles en el servidor, un número determinado de los mismos pueden entrar en ejecución, mientras que el resto debe esperar a que tenga suficientes recursos disponibles.

	Para conseguir este objetivo se usará StarCluster, el cual es un conjunto de herramientas para automatizar y simplificar la creación, configuración y gestión de máquinas virtuales usando Amazon EC2. StarCluster nos permitirá crear un entorno de computación en la nube, adecuado para programas distribuidos y paralelos. La estructura que monta se puede ver en la figura \ref{fig:StarClusterOverview} donde se crea un nodo \emph{Master} y varios nodos \emph{Worker}.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.65\textwidth]{img/scoverview.png}
  \caption{Arquitectura que se monta con StarCluster.}
  \label{fig:StarClusterOverview}
\end{figure}

\subsection{Instalando StarCluster}\label{sec:InstallStarCluster}
	Para la instalación de StarCluster basta con descargar la última versión de \url{https://pypi.python.org/pypi/StarCluster} que actualmente es la 0.95.6. Para descargar el archivo e instalar StarCluster basta con introducir en la terminal los siguientes comandos\cite{InstallingStarCluster}:
\begin{lstlisting}[style=minibash]
$ curl -O https://pypi.python.org/packages/source/S/StarCluster/StarCluster-0.95.6.tar.gz
$ tar xvzf StarCluster-0.95.6.tar.gz
$ rm StarCluster-0.95.6.tar.gz
$ cd StarCluster-0.95.6
$ sudo python distribute_setup.py
$ sudo python setup.py install
\end{lstlisting}

\subsection{Creando el archivo de configuración para StarCluster}
	El primer paso para la creación de un cluster es definir un archivo de configuración en el que se podrá indicar todos los parámetros necesarios. Por defecto, el archivo de configuración tiene que estar en \emph{\$HOME/.starcluster/config}, pero se le puede cambiar la localización con la variable de entorno \emph{STARCLUSTER\_CONFIG}\cite{StarClusterUserManual}.
\begin{lstlisting}[style=minibash]
$ echo "export STARCLUSTER_CONFIG=\"$PWD/starclusterconfig\"" >> $HOME/.bash_profile 
$ source $HOME/.bash_profile
\end{lstlisting}

Ahora procederemos a crear el archivo de configuración con el comando \emph{starcluster help} y nos mostrará diferentes opciones. Nosotros pulsaremos la opción 2 para que nos cree un archivo de configuración por defecto, donde indica la variable de entorno \emph{STARCLUSTER\_CONFIG}.
\begin{lstlisting}[style=minibash]
$ starcluster help
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

!!! ERROR - config file /Users/flanciskinho/Documents/Facultad/2014_2015/ICS/Practicas/05_Optativa/starclusterconfig does not exist

Options:
--------
[1] Show the StarCluster config template
[2] Write config template to /Users/flanciskinho/Documents/Facultad/2014_2015/ICS/Practicas/05_Optativa/starclusterconfig
[q] Quit

Please enter your selection: 2
>>> Config template written to /Users/flanciskinho/Documents/Facultad/2014_2015/ICS/Practicas/05_Optativa/starclusterconfig
>>> Please customize the config template
\end{lstlisting} 

	El primer paso que debemos hacer es la configuración de las credenciales de AWS en el fichero de configuración. Para ello dentro del archivo nos movemos a la sección \emph{[aws\_info]}. Las credenciales se han de indicar en los parámetros \emph{AWS\_ACCESS\_KEY\_ID}, \emph{AWS\_SECRET\_ACCESS\_KEY} y \emph{AWS\_USER\_ID}.
	
	Además de indicar las credenciales de AWS también tenemos que indicar el ``par de claves'' de Amazon EC2 en la sección [keypair]. Esto es necesario para que StarCluster se pueda conectar a las instancias y pueda configurarlas. Se puede crear varias secciones de par de claves. En la siguiente sección de configuración se muestra un ejemplo de la configuración:
\begin{lstlisting}
[key mykey1]
 KEY_LOCATION=./Keypair_AWS/01-francisco-cedron-keypair.pem
[key mykey2]
 KEY_LOCATION=./Keypair_AWS/02-francisco-cedron-keypair.pem
\end{lstlisting}

	A pesar de que en la sección [aws\_info] se tienen que indicar la configuración de las credenciales de AWS, en esta sección se puede configurar más cosas como la región donde estarán las instancias o la región donde está el servicio de S3 para el almacenamiento. Para indicar la región donde se lanzarán las instancias, hay que indicarlos con los parámetros \emph{aws\_region\_name} y \emph{aws\_region\_host} en la tabla \ref{tab:ec2-endpoint} se muestra como se deben modificar actualmente esos valores, aunque en la dirección \url{http://docs.aws.amazon.com/general/latest/gr/rande.html#ec2_region} aparecen los más recientes. Para indicar el lugar donde se aloja el servicio que se usará en Amazon S3, hay que modificar el parámetro \emph{aws\_s3\_host} con algunos de los valores de la tabla \ref{tab:s3-endpoint}, aunque en la dirección \url{http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region} aparecen los valores más recientes que pueden tomar. La opción que coge por defecto \emph{aws\_s3\_host} si no se especifica ningún valor, es \emph{s3.amazonaws.com} que indica que se usará la región del Norte de Virginia o la del Pacífico Noroeste.

\begin{table}[h]
	\begin{center}
		\caption{Regiones disponibles para Amazon EC2.}
		\begin{tabular}{|l|c|c|}
\hline
\textbf{Lugar} &
\textbf{aws\_region\_name} &
\textbf{aws\_region\_host} \\ \hline
\hline
US East (N. Virgina)          & us-east-1          &  ec2.us-east-1.amazonaws.com \\ \hline
US West (Oregon)             & us-west-2         &  ec2.us-west-2.amazonaws.com \\ \hline
US West (N. California)      & us-west-1         &  ec2.us-west-1.amazonaws.com \\ \hline
EU (Ireland)                       & eu-west-1         &  ec2.eu-west-1.amazonaws.com \\ \hline
EU (Frankfurt)                   & eu-central-1      & ec2.eu-central-1.amazonaws.com \\ \hline
Asia Pacific (Singapore)     & ap-southeast-1 & ec2.ap-southeast-1.amazonaws.com \\ \hline
Asia Pacific (Sydney)         & ap-southeast-2 & ec2.ap-southeast-2.amazonaws.com \\ \hline
Asia Pacific (Tokyo)            & ap-northeast-1 & ec2.ap-northeast-1.amazonaws.com \\ \hline
South America (São Paulo) & sa-east-1         & ec2.as-east-1.amazonaws.com \\ \hline
		\end{tabular}
		\label{tab:ec2-endpoint}
	\end{center}
\end{table}

\begin{table}[h]
	\begin{center}
		\caption{Regiones disponibles para Amazon S3.}
		\begin{tabular}{|l|c|}
\hline
\textbf{Lugar} &
\textbf{aws\_s3\_host} \\ \hline
\hline
US West (Oregon)            &  s3-us-west-2.amazonaws.com \\ \hline
US West (N. California)     &  s3-us-west-1.amazonaws.com \\ \hline
EU (Ireland)                       &  s3-eu-west-1.amazonaws.com \\ \hline
EU (Frankfurt)                   & s3-eu-central-1.amazonaws.com \\ \hline
Asia Pacific (Singapore)     & s3-ap-southeast-1.amazonaws.com \\ \hline
Asia Pacific (Sydney)         & s3-ap-southeast-2.amazonaws.com \\ \hline
Asia Pacific (Tokyo)            & s3-ap-northeast-1.amazonaws.com \\ \hline
South America (São Paulo) & s3-as-east-1.amazonaws.com \\ \hline
		\end{tabular}
		\label{tab:s3-endpoint}
	\end{center}
\end{table}

	Dentro de la sección de [aws\_info] también se puede especificar la configuración para usar un proxy para conectarse a AWS. Para ello hay que incluir los siguientes parámetros:
{\setlength{\parskip}{0mm} \begin{itemize}
{\setlength{\parskip}{0mm}
	\item 	\emph{aws\_proxy}: Es el nombre del host proxy que se empleará
	\item 	\emph{aws\_proxy\_port}: Es el número de puerto que utilizará el proxy para conectarse
	\item 	\emph{aws\_proxy\_user}: Es el nombre de usuario para utilizar cuando se autentique con el proxy
	\item 	\emph{aws\_proxy\_pass}: Es el contraseña a usar cuando se autentica 
}\end{itemize} }

	Con StarCluster se pueden usar los volúmenes de Amazon EBS para dar almacenamiento persistente al cluster. Si se quieren usar los volúmenes EBS con StarCluster hay que definir la sección para \emph{[volume]} en el archivo de configuración para cada volumen que se quiera usar. En la siguiente sección del archivo de configuración se muestra un ejemplo:
\begin{lstlisting}
[volume voldata1]
 volume_id=vol-c9999999
 mount_path=/home
[volume voldata2]
volume_id=vol-c8888888
mount_path=/scratch
\end{lstlisting}

	Cuando se inicia el cluster, cada nodo se añade a un grupo de seguridad común que está creado por StarCluster y tiene un nombre de la forma @sc-\textbf{cluster\_tag} donde \textbf{cluster\_tag} es el nombre que se pasa por parámetro al iniciar el cluster. Por defecto agrega un permiso para este grupo de seguridad, que permite el acceso al puerto ssh (22) desde cualquier dirección ip. Esta regla es necesaria para que StarCluster pueda conectarse a las instancias y configurarlas. Si necesita añadir más reglas en el grupo de seguridad del cluster, se puede hacer en el fichero de configuración en la sección [permission].  En la siguiente sección del archivo de configuración se muestra un ejemplo:
\begin{lstlisting}
[permission www]
 from_port = 80
to_port = 80
[permission ftp]
 from_port = 21
to_port = 21
cidr_ip = 66.249.90.104/32
[permission range]
from_port = 20
to_port = 53
\end{lstlisting}

	Además también se tiene soporte para plugins\cite{SystemPlugin} que se puede especificar en la sección de [plugin].

	Antes de crear un cluster con StarCluster sobre Amazon EC2, se tiene que especificar una plantilla que contenga toda la configuración necesaria para la creación del cluster. Para ello hay que crear una sección \emph{[cluster]} en el archivo de configuración. Una vez que tienes una plantilla definida para el cluster se pueden lanzar diversos clusters usando la misma plantilla. Las parámetros de configuración son los siguientes\footnote{Los parámetros que aparecen en negrita son obligatorios}:
{\setlength{\parskip}{0mm} \begin{itemize}
{\setlength{\parskip}{0mm}
	\item 	\textbf{\emph{keyname}}: Es el ``par de claves'' que se usará para el cluster. Para ello tiene que estar definido en la sección [keypair]
	\item \textbf{\emph{cluster\_size}}: El número de nodos que tendrá el cluster (se incluye también el nodo maestro).
	\item \textbf{\emph{node\_image\_id}}: El identificador del AMI que usarán en los nodos de trabajo
	\item \textbf{\emph{node\_instance\_type}}: Es el tipo de instancia que usarán en los nodos de trabajo
	\item \emph{cluster\_user}: El usuario que crear el cluster (por defecto es \emph{sgeadmin})
	\item \emph{cluster\_shell}: El shell del cluster (por defecto es \emph{bash}, aunque las opciones que puedes especificar son \emph{bash}, \emph{zsh}, \emph{csh}, \emph{ksh}, \emph{tcsh})
	\item \emph{dns\_prefix}: Si se especifica este valor el nombre dns de los nodos empezarán con el valor que se especifique aquí
	\item \emph{master\_image\_id}: El identificador de la AMI que se usará para el nodo master (si no se especifica se usa el que se haya indicado en node\_image\_id)
	\item \emph{master\_instance\_type}: Es el tipo de instancia que se usará para el nodo master (si no se especifica se usa el que se haya indicado en node\_instance\_type)
	\item \emph{userdata\_scripts}: Lista de los scripts que se usarán cuando se lancen las instancias
	\item \emph{volumes}: Lista de volúmenes EBS para adjuntar al servicio NFS compartido en el cluster (cada volumen tendrá que estar especificado en la sección [volumen])
	\item \emph{plugin}: Lista de plugins para StarCluster a usar cuando se lance el cluster (cada plugin tiene que estar definido en la sección [plugin])
	\item \emph{permissions}: Lista de permisos a aplicar al grupo de seguridad del cluster (cada permiso tendrá que estar especificado en la sección [permissions])
	\item \emph{spot\_bid}: Usar siempre las instancias \textit{spot}\cite{SpotInstancesEC2} con esta plantilla del cluster
	\item \emph{force\_spot\_master}: Por motivos de estabilidad el nodo master no es una instancia \textit{spot} pero con este parámetro se puede forzar a que lo sea
	\item \emph{availability\_zone}: Lanza todas las instancias del cluster en una zona de disponibilidad (por defecto usa cualquier zona disponible)
	\item \emph{dissable\_queue}: Desactiva la instalación y la configuración del \textit{Open Grid Scheduler}\cite{OpenGridScheduler}
	\item \emph{dissable\_cloudinit}: No usa \emph{cloudinit}\cite{CloudInit} para las cuentas del cluster (solo requerido si se usa cloudinit)
	\item \emph{subnet\_id}: La subred del VPC para usar cuando se lanzan las instancias del cluster
	\item \emph{public\_ips}: Automáticamente asigna una dirección de IP pública a todas las instancias del cluster (por defecto es falso)
}\end{itemize} }

	StarCluster no limita a usar una única plantilla en el archivo de configuración permitiendo definir múltiples plantillas en el mismo archivo de configuración creando varias secciones [cluster]. Además como la definición de estas plantillas puede ser algo repetitivo es posible crear una plantilla a partir de la definición de otra. En la siguiente sección del archivo de configuración se muestra un ejemplo:
\begin{lstlisting}
[cluster proveCluster]
keyname = mykeypair1
cluster_size = 2
cluster_user = sgeadmin
cluster_shell = bash
master_image_id = ami-0330d16a
master_instance_type = m1.small
node_image_id = ami-0330d16a
node_instance_type = m1.small

[cluster productionCluster]
extends = proveCluster
keyname = mykeypair2
cluster_size = 10
node_instance_type = c1.xlarge
\end{lstlisting}
	Como en el archivo de configuración se pueden tener varias plantillas de cluster, es posible definir una como predeterminada, que se hace en la sección [global] de la siguiente manera:
\begin{lstlisting}
[global]
default_template = proveCluster
\end{lstlisting}

	Las nuevas cuentas de AWS utilizan una VPC por defecto y  StarCluster soporta esta configuración. Sin embargo, si ya se quiere usar una VPC ya personalizada por el usuario, se puede especificar en el parámetro \emph{subnet\_id}. Por defecto StarCluster no asigna IP públicas a todas las instancias y esa es la razón por la que crea todas las instancias en una misma VPC, para poder conectarse a todos los nodos. StarCluster no asigna direcciones IP públicas por dos sencillas razones:
{\setlength{\parskip}{0mm}
\begin{enumerate}
{\setlength{\parskip}{0mm}
	\item Eso ``abriría'' la VPC a internet, lo que es un riesgo para la seguridad
	\item Se requiere una configuración especial del VPC
}
\end{enumerate} 
}
	Además StarCluster te indica que si usas una VPC que no sea por la de defecto

\subsection{Archivo de configuración a usar}
	En la figura \ref{fig:configfile} podemos ver el archivo de configuración usado para StarCluster. Como podemos ver la configuración es muy corta. Basta con especificar el acceso (credenciales para AWS y el ``par de claves'') y las características del cluster.
	
	Podemos ver que se crearon tres plantillas diferentes de cluster:
{\setlength{\parskip}{0mm}
\begin{enumerate}
{\setlength{\parskip}{0mm}
	\item \textbf{provecluster}: Es un cluster en el que se uso para probar el funcionamiento de de la arquitectura creada por StarCluster. Este se uso con instancias de tipo t2.medium por lo baratas que son y para comprobar el funcionamiento son válidas.
	\item \textbf{mediumcluster}: Es el cluster que se creo con instancias de propósito general. La instancias m3.xlarge usan 4 vCPU con lo que al tener 10 nodos disponemos un total de 40 CPU virtuales.
	\item \textbf{largecluster}: Es el cluster que se creo con instancias optimizadas para cálculo. La instancias c3.2xlarge usan 8 vCPU con lo que al tener 5 nodos disponemos un total de 40 CPU virtuales.
}
\end{enumerate}
}

\begin{figure}[h]
\centering
\begin{lstlisting}
####################################
## StarCluster Configuration File ##
####################################
[global]
DEFAULT_TEMPLATE=provecluster

#############################################
## AWS Credentials and Connection Settings ##
#############################################
[aws info]
AWS_ACCESS_KEY_ID=AKIAIBAREPSSNKLL43KA
AWS_SECRET_ACCESS_KEY=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

AWS_USER_ID=starcluster
AWS_REGION_NAME = us-east-1
AWS_REGION_HOST = ec2.us-east-1.amazonaws.com

###########################
## Defining EC2 Keypairs ##
###########################
[key mykey]
KEY_LOCATION=~/.ssh/mykey.rsa

################################
## Defining Cluster Templates ##
################################
[cluster provecluster]
KEYNAME=mykey
CLUSTER_SIZE = 4
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
NODE_IMAGE_ID = ami-6b211202
NODE_INSTANCE_TYPE = t2.medium

###########################################
## Defining Additional Cluster Templates ##
###########################################
[cluster mediumcluster]
EXTENDS=provecluster
NODE_INSTANCE_TYPE = m3.xlarge
CLUSTER_SIZE=10
PLUGINS=mpich2

[cluster largecluster]
EXTENDS=mediumcluster
NODE_INSTANCE_TYPE=c3.2xlarge
CLUSTER_SIZE=5

######################
## Built-in Plugins ##
######################
[plugin mpich2]
SETUP_CLASS = starcluster.plugins.mpich2.MPICH2Setup
\end{lstlisting}
\caption{Archivo de configuración usado para StarCluster.}
\label{fig:configfile}
\end{figure}	

\clearpage
\subsection{Usando el cluster}
	Una vez creado el archivo de configuración, toca crear el cluster con el comando \emph{starcluster start namecluster}, donde \emph{namecluster} es una etiqueta que le damos al cluster que vamos a crear. A continuación, vemos la salida que nos devolvió el comando, y en las imágenes \ref{fig:ClusterDePruebasSG} y \ref{fig:ClusterDePruebasNODES} podemos ver el grupo de seguridad y las instancias en el panel de Amazon EC2 que se crearon para el cluster.
\begin{lstlisting}[style=miniBash]
$ starcluster start ClusterDePruebas
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

>>> Using default cluster template: provecluster
>>> Validating cluster template settings...
>>> Cluster template settings are valid
>>> Starting cluster...
>>> Launching a 4-node cluster...
>>> Creating security group @sc-ClusterDePruebas...
Reservation:r-1ce20cf7
>>> Waiting for instances to propagate...
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Waiting for cluster to come up... (updating every 30s)
>>> Waiting for all nodes to be in a 'running' state...
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Waiting for SSH to come up on all nodes...
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Waiting for cluster to come up took 1.682 mins
>>> The master node is ec2-54-175-7-72.compute-1.amazonaws.com
>>> Configuring cluster...
>>> Running plugin starcluster.clustersetup.DefaultClusterSetup
>>> Configuring hostnames...
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Creating cluster user: sgeadmin (uid: 1001, gid: 1001)
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Configuring scratch space for user(s): sgeadmin
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Configuring /etc/hosts on each node
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Starting NFS server on master
>>> Configuring NFS exports path(s):
/home
>>> Mounting all NFS export path(s) on 3 worker node(s)
3/3 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Setting up NFS took 0.229 mins
>>> Configuring passwordless ssh for root
>>> Configuring passwordless ssh for sgeadmin
>>> Running plugin starcluster.plugins.sge.SGEPlugin
>>> Configuring SGE...
>>> Configuring NFS exports path(s):
/opt/sge6
>>> Mounting all NFS export path(s) on 3 worker node(s)
3/3 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Setting up NFS took 0.168 mins
>>> Installing Sun Grid Engine...
3/3 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Creating SGE parallel environment 'orte'
4/4 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Adding parallel environment 'orte' to queue 'all.q'
>>> Configuring cluster took 2.084 mins
>>> Starting cluster took 3.858 mins

The cluster is now ready to use. To login to the master node
as root, run:

    $ starcluster sshmaster ClusterDePruebas

If you're having issues with the cluster you can reboot the
instances and completely reconfigure the cluster from
scratch using:

    $ starcluster restart ClusterDePruebas

When you're finished using the cluster and wish to terminate
it and stop paying for service:

    $ starcluster terminate ClusterDePruebas

Alternatively, if the cluster uses EBS instances, you can
use the 'stop' command to shutdown all nodes and put them
into a 'stopped' state preserving the EBS volumes backing
the nodes:

    $ starcluster stop ClusterDePruebas

WARNING: Any data stored in ephemeral storage (usually /mnt)
will be lost!

You can activate a 'stopped' cluster by passing the -x
option to the 'start' command:

    $ starcluster start -x ClusterDePruebas

This will start all 'stopped' nodes and reconfigure the
cluster.
\end{lstlisting}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.75\textwidth]{img/ClusterDePruebasSG.png}
  \caption{El grupo de seguridad que creó StarCluster para el cluster}
  \label{fig:ClusterDePruebasSG}
\end{figure}
\begin{figure}[h]
  \centering
    \includegraphics[width=0.75\textwidth]{img/ClusterDePruebasNODES.png}
  \caption{Las instancias que aparecen en el panel de Amazon EC2 son los nodos del cluster}
  \label{fig:ClusterDePruebasNODES}
\end{figure}

	Una vez tenemos creado el cluste,r podemos consultar los clusters que hemos creado con el siguiente comando:
\begin{lstlisting}[style=miniBash]
$ starcluster listclusters
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

-------------------------------------------------------
ClusterDePruebas (security group: @sc-ClusterDePruebas)
-------------------------------------------------------
Launch time: 2014-12-12 05:25:49
Uptime: 0 days, 00:06:26
VPC: vpc-7f9a281a
Subnet: subnet-e7b46790
Zone: us-east-1c
Keypair: mykey
EBS volumes: N/A
Cluster nodes:
     master running i-68627582 ec2-54-175-7-72.compute-1.amazonaws.com
    node001 running i-69627583 ec2-54-175-7-92.compute-1.amazonaws.com
    node002 running i-6e627584 ec2-54-175-7-112.compute-1.amazonaws.com
    node003 running i-6f627585 ec2-54-175-7-109.compute-1.amazonaws.com
Total nodes: 4
\end{lstlisting}

	Aunque podemos mirar a través del portal web que se ha creado en cada momento, StarCluster nos ofrece herramientas para conectarnos al cluster. Por ejemplo, para conectarnos como root bastaría con usar el comando \emph{starcluster sshmaster ClusterDePruebas}, donde el primer argumento indica que nos queremos conectar al nodo master y el segundo es a qué cluster nos queremos conectar. Para conectarnos con el usuario que especificamos en el archivo de configuración (\emph{sgeadmin}), tendremos que usar el siguiente comando:
\begin{lstlisting}[style=miniBash]
$ starcluster sshmaster -u sgeadmin ClusterDePruebas
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

The authenticity of host 'ec2-54-175-7-72.compute-1.amazonaws.com (54.175.7.72)' can't be established.
ECDSA key fingerprint is f3:73:ff:c0:c5:f1:c1:5c:bf:f1:ea:c7:a3:7f:cf:a0.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ec2-54-175-7-72.compute-1.amazonaws.com,54.175.7.72' (ECDSA) to the list of known hosts.

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

          _                 _           _
__/\_____| |_ __ _ _ __ ___| |_   _ ___| |_ ___ _ __
\    / __| __/ _` | '__/ __| | | | / __| __/ _ \ '__|
/_  _\__ \ || (_| | | | (__| | |_| \__ \ ||  __/ |
  \/ |___/\__\__,_|_|  \___|_|\__,_|___/\__\___|_|

StarCluster Ubuntu 13.04 AMI
Software Tools for Academics and Researchers (STAR)
Homepage: http://star.mit.edu/cluster
Documentation: http://star.mit.edu/cluster/docs/latest
Code: https://github.com/jtriley/StarCluster
Mailing list: http://star.mit.edu/cluster/mailinglist.html

This AMI Contains:

  * NVIDIA Driver 331.38
  * NVIDIA CUDA Toolkit 5.5.22
  * PyCuda 2013.1.1 and PyOpenCL 2013.2
  * MAGMA 1.4.1
  * Intel Ethernet Driver 2.11.3 (ixgbevf)
  * Open Grid Scheduler (OGS - formerly SGE) queuing system
  * Condor workload management system
  * OpenMPI compiled with Open Grid Scheduler support
  * OpenBLAS - Highly optimized Basic Linear Algebra Routines
  * NumPy/SciPy linked against OpenBlas
  * Pandas - Data Analysis Library
  * IPython 1.1.0 with parallel and notebook support
  * Julia 0.3pre
  * and more! (use 'dpkg -l' to show all installed packages)

Open Grid Scheduler/Condor cheat sheet:

  * qstat/condor_q - show status of batch jobs
  * qhost/condor_status- show status of hosts, queues, and jobs
  * qsub/condor_submit - submit batch jobs (e.g. qsub -cwd ./job.sh)
  * qdel/condor_rm - delete batch jobs (e.g. qdel 7)
  * qconf - configure Open Grid Scheduler system

Current System Stats:

  System load:  0.0               Processes:           94
  Usage of /:   63.3% of 7.74GB   Users logged in:     0
  Memory usage: 11%               IP address for eth0: 172.31.28.11
  Swap usage:   0%

    https://landscape.canonical.com/
sgeadmin@master:~$ qhost
HOSTNAME                ARCH         NCPU  LOAD  MEMTOT  MEMUSE  SWAPTO  SWAPUS
-------------------------------------------------------------------------------
global                  -               -     -       -       -       -       -
master                  linux-x64       1  0.12  993.6M  103.1M     0.0     0.0
node001                 linux-x64       1  0.09  993.6M   89.8M     0.0     0.0
node002                 linux-x64       1  0.10  993.6M   89.5M     0.0     0.0
node003                 linux-x64       1  0.13  993.6M   89.3M     0.0     0.0
sgeadmin@master:~$ cat /etc/hosts
127.0.0.1 localhost

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
172.31.28.11 master
172.31.28.14 node001
172.31.28.12 node002
172.31.28.13 node003
sgeadmin@master:~$ logout
Connection to ec2-54-175-7-72.compute-1.amazonaws.com closed.
\end{lstlisting}

	Para pasar información al cluster, podemos usar el comando \emph{starcluster put clustertag pathlocal pathremote} donde \emph{clustertag} es la etiqueta que le pusimos al cluster, \emph{pathlocal} es la ruta del archivo/directorio\footnote{Tanto el comando \emph{starcluster put} como \emph{starcluster get} soportan recursividad} en el equipo local y \emph{pathremote} es la ruta donde situar el archivo/directorio en el servidor. Por defecto, este comando opera en el nodo master como root, pero se pueden usar las opciones \emph{--user username} donde \emph{username} es el nombre de usuario que realizará la operación y con \emph{--node nodename} especificamos mediante el parámetro \emph{nodename} el nombre del nodo contra el cual se realiza la operación. Para recuperar información del servidor es con el comando \emph{starcluster get clustertag pathremote pathlocal} y tiene un funcionamiento análogo a \emph{starcluster put} admitiendo las mismas opciones.
\begin{lstlisting}[style=miniBash]
$ starcluster put ClusterDePruebas --user sgeadmin /tmp/paralelizado.c code.c
StarCluster  - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

paralelizado.c 100% |||||||||||||||||||||||||||||||| Time: 00:00:00   3.25 M/s
$ starcluster get ClusterDePruebas --user sgeadmin code /tmp/code.c
StarCluster  - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

/tmp/code.c 100% |||||||||||||||||||||||||||||||| Time: 00:00:00   44.75 K/s
$ diff /tmp/paralelizado.c /tmp/code.c
$
\end{lstlisting}

	StarCluster proporciona una instrucción para ver la lista de clusters que tenemos en ese momento ejecutándose. Para ver cuáles son, basta con introducir el siguiente comando:
\begin{lstlisting}[style=miniBash]	
$ starcluster listclusters
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

-------------------------------------------------------
ClusterDePruebas (security group: @sc-ClusterDePruebas)
-------------------------------------------------------
Launch time: 2014-12-12 05:25:49
Uptime: 0 days, 00:06:26
VPC: vpc-7f9a281a
Subnet: subnet-e7b46790
Zone: us-east-1c
Keypair: mykey
EBS volumes: N/A
Cluster nodes:
     master running i-68627582 ec2-54-175-7-72.compute-1.amazonaws.com
    node001 running i-69627583 ec2-54-175-7-92.compute-1.amazonaws.com
    node002 running i-6e627584 ec2-54-175-7-112.compute-1.amazonaws.com
    node003 running i-6f627585 ec2-54-175-7-109.compute-1.amazonaws.com
Total nodes: 4

\end{lstlisting}
	
	Para finalizar el cluster, basta con introducir la siguiente instrucción:
\begin{lstlisting}[style=miniBash]
$ starcluster terminate ClusterDePruebas
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

Terminate EBS cluster ClusterDePruebas (y/n)? y
>>> Running plugin starcluster.plugins.sge.SGEPlugin
>>> Running plugin starcluster.clustersetup.DefaultClusterSetup
>>> Terminating node: master (i-68627582)
>>> Terminating node: node001 (i-69627583)
>>> Terminating node: node002 (i-6e627584)
>>> Terminating node: node003 (i-6f627585)
>>> Waiting for cluster to terminate... 
>>> Removing security group: @sc-ClusterDePruebas 
\end{lstlisting}

\subsubsection{Iniciando un cluster a partir de una plantilla que no sea la de por defecto}
	En el archivo de configuración pueden existir varias plantillas para la creación de un cluster, pero a la hora de ejecutar el comando \emph{starcluster start clustername} se utiliza la plantilla que se estableció por defecto dentro de la sección [global]. Si necesitamos crear otra plantilla tendremos que especificarlo  con el parámetro \emph{--cluster-template=CLUSTER\_TEMPLATE}, donde \emph{CLUSTER\_TEMPLATE} es la plantilla que queremos cargar. El siguiente comando inicia la plantilla \emph{mediumcluster}:
\begin{lstlisting}[style=miniBash]
$ starcluster start --cluster-template=mediumcluster mpicluster
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

>>> Validating cluster template settings...
>>> Cluster template settings are valid
>>> Starting cluster...
>>> Launching a 10-node cluster...
>>> Creating security group @sc-mpicluster...
Reservation:r-79c02e92
>>> Waiting for instances to propagate...
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Waiting for cluster to come up... (updating every 30s)
>>> Waiting for all nodes to be in a 'running' state...
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Waiting for SSH to come up on all nodes...
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Waiting for cluster to come up took 1.814 mins
>>> The master node is ec2-54-174-239-221.compute-1.amazonaws.com
>>> Configuring cluster...
>>> Running plugin starcluster.clustersetup.DefaultClusterSetup
>>> Configuring hostnames...
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Creating cluster user: sgeadmin (uid: 1001, gid: 1001)
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Configuring scratch space for user(s): sgeadmin
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Configuring /etc/hosts on each node
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Starting NFS server on master
>>> Configuring NFS exports path(s):
/home
>>> Mounting all NFS export path(s) on 9 worker node(s)
9/9 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Setting up NFS took 0.257 mins
>>> Configuring passwordless ssh for root
>>> Configuring passwordless ssh for sgeadmin
>>> Running plugin starcluster.plugins.sge.SGEPlugin
>>> Configuring SGE...
>>> Configuring NFS exports path(s):
/opt/sge6
>>> Mounting all NFS export path(s) on 9 worker node(s)
9/9 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Setting up NFS took 0.203 mins
>>> Installing Sun Grid Engine...
9/9 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Creating SGE parallel environment 'orte'
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Adding parallel environment 'orte' to queue 'all.q'
>>> Running plugin mpich2
>>> Creating MPICH2 hosts file
>>> Configuring MPICH2 profile
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> Setting MPICH2 as default MPI on all nodes
10/10 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  
>>> MPICH2 is now ready to use
>>> Use mpicc, mpif90, mpirun, etc. to compile and run your MPI apps
>>> Configuring cluster took 2.117 mins
>>> Starting cluster took 4.020 mins

The cluster is now ready to use. To login to the master node
as root, run:

    $ starcluster sshmaster mpicluster

If you're having issues with the cluster you can reboot the
instances and completely reconfigure the cluster from
scratch using:

    $ starcluster restart mpicluster

When you're finished using the cluster and wish to terminate
it and stop paying for service:

    $ starcluster terminate mpicluster

Alternatively, if the cluster uses EBS instances, you can
use the 'stop' command to shutdown all nodes and put them
into a 'stopped' state preserving the EBS volumes backing
the nodes:

    $ starcluster stop mpicluster

WARNING: Any data stored in ephemeral storage (usually /mnt)
will be lost!

You can activate a 'stopped' cluster by passing the -x
option to the 'start' command:

    $ starcluster start -x mpicluster

This will start all 'stopped' nodes and reconfigure the
cluster.
\end{lstlisting}
	
	

\section{Ejecución del programa paralelo en un cluster}\label{sec:ejStarCluster}

\subsection{Tipo de instancias de propósito general}\label{sec:ejM3}
	Para probar la ejecución del programa paralelo se ha usado una configuración con 10 nodos, cuyo tipo de instancia es el \emph{m3.xlarge} que tienen 4 VCPUs cada una. En la tabla \ref{tab:resultadosClusterM} se pueden ver los resultados que se obtuvieron al ejecutar el mismo programa con las mismas entradas en el cluster formado en Amazon EC2 con StarCluster.

	Si nos fijamos en los resultados obtenidos en la tabla \ref{tab:resultadosClusterM}, puede llamar la atención como varían los valores y que no siguen una distribución como la que aparece en los resultados al ejecutarlos en local. Podemos ver que el speedup que se consigue al usar cinco procesos es superior a lo que sería lo óptimo. Esto se debe a que las instancias que se ejecutan son máquinas virtuales de las que tenemos el control, pero no de la máquina física sobre la que se ejecuten pudiendo haber otras instancias de otros usuarios ejecutándose en la misma máquina física de tal manera que no tenemos la CPU física para nuestros procesos como pasaba en el caso de ejecutarse en local. Esto es lo que puede explicar los datos atípicos que parecen mejorar o empeorar los resultados. Además,  los casos en los que los resultados parecen que empeoran mucho  se debe a la comunicación entre procesos de tal manera que si los datos tienen que viajar entre máquinas, es muy probable que exista otro tráfico que haga que los mensajes de comunicación del programa no se entreguen todo lo rápido que podría ser.
	
%http://aws.amazon.com/es/ec2/instance-types/#instance-type-matrix
\begin{table}[h]
	\begin{center}
		\caption{Resultados de ejecutar el programa paralelo en un cluster en Amazon EC2 usando tipos instancias de propósito general con 1, 2, 3, 4, 5, 10, 20, 30 y 40 procesos.}
		\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Procesos} &
\textbf{Tiempo} &
\textbf{Speedup} &
\textbf{Eficiencia} \\ \hline
\hline
 1 &234,867 sg &  1          & 100\% \\ \hline
 2 &132,390 sg &  1,7740 & 88,36\% \\ \hline 
 3 & 97,506 sg &  2,4087 & 80,29\% \\ \hline
 4 & 78,707 sg &  2,9840 & 74,60\% \\ \hline
 5 & 42,560 sg &  5,5184 & 110,3\% \\ \hline
10 & 21,230 sg & 11,0629 & 110,6\% \\ \hline
20 & 16,990 sg &  13,8238& 69,11\% \\ \hline
30 & 11,115 sg & 21,1306 & 70,43\% \\ \hline
40 &  8,411 sg &  27,9237& 93,07\% \\ \hline
		\end{tabular}
		\label{tab:resultadosClusterM}
	\end{center}
\end{table}

\subsection{Tipo de instancias optimizadas para cálculos}\label{sec:ejC3}
	Para probar la ejecución del programa paralelo se ha usado una configuración con 5 nodos, cuyo tipo de instancia es el \emph{c3.2xlarge} que tienen 8 VCPUs cada una. En la tabla \ref{tab:resultadosClusterC} se pueden ver los resultados que se obtuvieron al ejecutar el mismo programa con las mismas entradas en el cluster formado en Amazon EC2 con StarCluster.

	Podemos ver que la eficiencia se mantiene casi constante durante las primeras ejecuciones mientras, que a partir de los 30 procesos empieza a bajar considerablemente. Esto se debe a que las llamadas que se usan de MPI son llamadas colectivas donde todos los procesos tienen que esperar a que todos los demás lleguen al mismo punto (como si fuese una llamada \emph{MPI\_Barrier}) para empezar a realizar la comunicación, y con 30 procesos ya es una cantidad considerable de procesos para sincronización (hay que darse cuenta de que la sincronización no tiene porque ser dentro de la misma máquina sino que puede ser con otras máquinas con lo que hay que hacer la sincronización a través de la red).

\begin{table}[h]
	\begin{center}
		\caption{Resultados de ejecutar el programa paralelo en un cluster en Amazon EC2 usando tipos de instancias  optimizadas para computo.}
		\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Procesos} &
\textbf{Tiempo} &
\textbf{Speedup} &
\textbf{Eficiencia} \\ \hline
\hline
 1 &153,85 sg &  1          & 100\% \\ \hline
 2 & 76,93 sg &  1,9999 & 99,99\% \\ \hline 
 3 & 51,233 sg &  3,0029 & 100,10\% \\ \hline 
 4 & 38,575 sg &  3,9883 & 99,71\% \\ \hline
 5 & 30,865 sg &  4,9846 & 99,69\% \\ \hline
10 & 15,392 sg &  9,9955 & 99,95\% \\ \hline
20 & 7,772 sg & 19,7954 & 98,98\% \\ \hline
30 & 7,461 sg &  20,6206& 68,74\% \\ \hline
40 & 5,818 sg & 26,4438 & 66,11\% \\ \hline
		\end{tabular}
		\label{tab:resultadosClusterC}
	\end{center}
\end{table}

\clearpage
\section{Conclusiones}\label{sec:Conclusion}

\subsection{Problemas al instalar StarCluster}

\subsubsection{Instalación en MAC OS X}
	StarCluster está disponible mediante \emph{PyPI} (Python Package Index), cuya herramienta se encuentra disponible tanto para Mac OS X como para sistemas Linux. Debido a esto, probamos a instalarlo en Mac OS X 10.9.4 como muestra en la sección \ref{sec:InstallStarCluster}, con lo que después de algún mensaje de warning, la instalación se completo sin dar ningún mensaje de error. Si configuramos el archivo de configuración para lanzar un cluster y intentábamos crearlo, nos daba el siguiente mensaje:
\begin{lstlisting}[style=miniBash]
$ starcluster start pruebas
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

>>> Using default cluster template: provecluster
>>> Validating cluster template settings...
!!! ERROR - Cluster settings are not valid:
!!! ERROR - key_location '/Users/flanciskinho/.ssh/AWS/mykey.rsa' does
!!! ERROR - not exist
\end{lstlisting}
Lo que nos indica el mensaje es que la clave que se ha indicado en el archivo de configuración no existe en Amazon EC2. Se probó crear la clave de tres formas distintas, pero consiguiendo siempre el mismo mensaje:
{\setlength{\parskip}{0mm}\begin{enumerate}
{\setlength{\parskip}{0mm}
	\item Creando la clave mediante el panel web de Amazon EC2
	\item Creando la clave usando el comando \emph{ssh-keygen -t rsa}
	\item Haciendo uso de starcluster con el comando \emph{starcluster createkey mykey -o ~/.ssh/mykey.rsa} que añade la clave pública en Amazon EC2 bajo el nombre \emph{mykey} y añade la clave privada \emph{mykey.rsa} en nuestro equipo
}
\end{enumerate}}

	La solución pasó por instalar StarCluster en un sistema basado en linux (más concretamente en Ubuntu 14.04).

\subsection{Resultados}\label{sec:ConclusionResultados}
	Las pruebas para la ejecución del problema propuesto en esta práctica se han realizado con tres configuraciones diferentes:
{\setlength{\parskip}{0mm}\begin{enumerate}
{\setlength{\parskip}{0mm}
	\item Una configuración en una máquina local (sección \ref{sec:ejLocal}).
	\item Una configuración con un cluster en Amazon EC2 con instancias de propósito general (sección \ref{sec:ejM3}).
	\item Una configuración con un cluster en Amazon EC2 con instancias optimizadas para cálculo (sección \ref{sec:ejC3}).
}
\end{enumerate}}
	Si comparamos los tiempos de las ejecuciones en ambas configuraciones (véase la figura \ref{fig:time}), podemos ver que para una cantidad menor de procesos se ejecuta más rápido en la máquina local que usando las instancias de Amazon EC2, lo cual es lógico porque la máquina local tiene una velocidad de reloj de 3,7 GHz mientras que las instancias para propósito general tienen una velocidad de 2,5 GHZ y las optimizadas para cálculo 2,8 GHz\footnote{ Velocidad de reloj para los distintos tipos de instancia de Amazon EC2: \url{http://aws.amazon.com/es/ec2/instance-types/\#instance-type-matrix}}.


\begin{figure}[h]
  \centering
    \includegraphics[width=0.65\textwidth]{img/time.pdf}
  \caption{Diferencia de tiempos. Rojo: ejecución en Local. Naranja: Ejecución con instancias M3. Verde: Ejecución con instancias C3.}
  \label{fig:time}
\end{figure}

	A pesar de que con pocos procesos la máquina local es más rápida, tiene una limitación bastante importante: sólo dispone de 4 cores. Gracias a StarCluster pudimos montar un cluster con varias máquinas de manera rápida lo que nos proporcionaba el uso de una cantidad mayor de cores. Como se ve en la gráfica de tiempos (figura \ref{fig:time}), en los cluster pudimos lanzar más procesos de manera paralela, por eso el tiempo se acaba mejorando con respecto al tiempo conseguido en la máquina local. Una cosa que también se ve en la gráfica es que el tiempo de ejecución parece estabilizarse. Eso se explica mediante la ley de Amdahl\footnote{Ley de Amdahl: \url{http://es.wikipedia.org/wiki/Ley_de_Amdahl}} porque sólo mejoramos el tiempo de cómputo y no el tiempo de comunicación entre procesos, una cosa que se puede ver perfectamente en las gráficas de eficiencia y \emph{Speedup} (figuras \ref{fig:eficiencia} y \ref{fig:speedup} respectivamente).


\begin{figure}[h]
  \centering
    \includegraphics[width=0.65\textwidth]{img/speedup.pdf}
  \caption{Diferencia de speedup. Rojo: ejecución en Local. Naranja: Ejecución con instancias M3. Verde: Ejecución con instancias C3. Negro: Situación ideal.}
  \label{fig:speedup}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.65\textwidth]{img/eficiencia.pdf}
  \caption{Diferencia en la eficiencia. Rojo: ejecución en Local. Naranja: Ejecución con instancias M3. Verde: Ejecución con instancias C3. Negro: Situación ideal.}
  \label{fig:eficiencia}
\end{figure}

	Si nos fijamos en las gráficas de la eficiencia, podemos ver que hasta los 20 procesos la eficiencia en las instancias optimizadas para cálculo están muy cerca de la situación ideal, esto se debe a que son las que Amazon te ofrece para HPC\cite{AmazonHPC}. Sin embargo, para las instancias dedicadas para propósito general se ve una cosa que a priori es extraña. Esta situación en la que varía tanto la eficiencia se debe principalmente a que no podemos estar seguros de que el procesador de la máquina física sólo lo estemos usando nosotros.
	
	Con esto podemos concluir que, para pocos problemas y pequeños lo recomendable es ejecutarlos en una máquina en local, pero para problemas paralelizables que necesiten mucho computo (o para una gran cantidad de programas) compensa el uso de un cluster de computación como el que se puede crear con la herramienta de StarCluster. Además, es más recomendable usar las instancias optimizadas para cálculo, en lugar de las de propósito general, aunque las primeras sean más caras\footnote{Precio de las instancias: \url{http://aws.amazon.com/ec2/pricing/\#On-Demand\_Instances}} .

\clearpage
{\setlength{\parskip}{0mm} \listoftables } % Para que no ponga espacios entre las lineas de indice
{\setlength{\parskip}{0mm} \listoffigures} % Para que no ponga espacios entre las lineas de indice

% Bibliografía.
%-----------------------------------------------------------------
\clearpage

\renewcommand{\bibname}{Referencias}
\begin{thebibliography}{99}
\bibitem{WOODS}
Rafael C. González and Richard E. Woods
\emph{Digital Image Processing}
Addison-Wesley, USA
2nd Edition
1993
\bibitem{MPICH2}
Biblioteca MPI de software libre

\url{http://www.mcs.anl.gov/research/projects/mpich2/}
\bibitem{OpenMPI}
Biblioteca MPI de software libre

\url{http://www.open-mpi.org}
\bibitem{InstallingStarCluster}
Instalación de StarCluster

\url{http://star.mit.edu/cluster/docs/latest/installation.html}
\bibitem{StarClusterUserManual}
Manual de uso de StarCluster

\url{http://star.mit.edu/cluster/docs/latest/manual/index.html}
\bibitem{SystemPlugin}
Sistema de Plugin para StarCluster

\url{http://star.mit.edu/cluster/docs/latest/manual/plugins.html}
\bibitem{SpotInstancesEC2}
Instancias puntuales de Amazon EC2

\url{http://aws.amazon.com/es/ec2/purchasing-options/spot-instances/}
\bibitem{OpenGridScheduler}
Open Grid Scheduler. Documentación del Grid Engine

\url{http://gridscheduler.sourceforge.net/documentation.html}
\bibitem{CloudInit}
Documentación de cloudinit

\url{https://cloudinit.readthedocs.org/en/latest/}
\bibitem{AmazonHPC}
Informática de alto rendimiento

\url{http://aws.amazon.com/es/hpc/#/content/aws.amazon.com/es_ES/ec2/details}
\end{thebibliography}

\end{document}